{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport torch\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import model\n",
    "import data_io\n",
    "import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_error(prediction, target, minkowski_parameter=1.5):\n",
    "    \"\"\"\n",
    "    Minkowski error to be better deal with outlier errors\n",
    "    \"\"\"\n",
    "    loss = torch.mean((torch.abs(prediction - target))**minkowski_parameter)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "region=\"9999LEAU_std\"\n",
    "identifier=\"9999LEAU\"\n",
    "\n",
    "nlevs = 45\n",
    "in_features, nb_classes=(nlevs*4+3),(nlevs*2)\n",
    "nb_hidden_layer = 8\n",
    "hidden_size = 512\n",
    "mlp = model.MLP(in_features, nb_classes, nb_hidden_layer, hidden_size)\n",
    "# mlp = model.MLP_BN(in_features, nb_classes, nb_hidden_layer, hidden_size)\n",
    "pytorch_total_params = sum(p.numel() for p in mlp.parameters() if p.requires_grad)\n",
    "print(\"Number of traninable parameter: {0}\".format(pytorch_total_params))\n",
    "\n",
    "model_name = \"q_qadv_t_tadv_swtoa_lhf_shf_qtphys_{0}_lyr_{1}_in_{2}_out_{3}_hdn_{4}_epch_{5}_btch_{6}_mae_vlr.tar\".format(str(nb_hidden_layer).zfill(3),\n",
    "                                                                                    str(in_features).zfill(3),\n",
    "                                                                                    str(nb_classes).zfill(3),\n",
    "                                                                                    str(hidden_size).zfill(4),\n",
    "                                                                                    str(args.epochs).zfill(3),\n",
    "                                                                                    str(args.batch_size).zfill(5),\n",
    "                                                                                    identifier)\n",
    "optimizer =  torch.optim.Adam(mlp.parameters(), lr=1.e-3)\n",
    "# optimizer =  torch.optim.SGD(mlp.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# loss_function = torch.nn.MSELoss()\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "locations={ \"train_test_datadir\":\"/content/drive/My Drive/ML/data\",\n",
    "            \"model_loc\":\"/content/drive/My Drive/ML/data/model\"}\n",
    "\n",
    "mlp.to(device)\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in mlp.state_dict():\n",
    "    print(param_tensor, \"\\t\", mlp.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x3data, x2data, ydata, nlevs):\n",
    "        self.x3datasets = x3data\n",
    "        self.x2datasets = x2data\n",
    "        self.ydatasets = ydata\n",
    "        self.nlevs = nlevs\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x3 = [torch.tensor(d[i,:self.nlevs]) for d in self.x3datasets]\n",
    "        x2 = [torch.tensor(d[i]) for d in self.x2datasets]\n",
    "        x = torch.cat(x3+x2,dim=0)\n",
    "        y = torch.cat([torch.tensor(d[i,:self.nlevs]) for d in self.ydatasets],dim=0)\n",
    "        return (x.to(device),y.to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.x2datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data = data_io.Data_IO(region, locations)\n",
    "\n",
    "x2d  = [nn_data.sw_toa_train, nn_data.lhf_train, nn_data.shf_train]\n",
    "x2d_test  = [nn_data.sw_toa_test, nn_data.lhf_test, nn_data.shf_test]\n",
    "x3d  = [nn_data.q_tot_train, nn_data.q_tot_adv_train, nn_data.theta_train, nn_data.theta_adv_train]\n",
    "x3d_test  = [nn_data.q_tot_test, nn_data.q_tot_adv_test, nn_data.theta_test, nn_data.theta_adv_test]\n",
    "y = [nn_data.qphys_train, nn_data.theta_phys_train]\n",
    "y_test = [nn_data.qphys_test, nn_data.theta_phys_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(x3d,x2d,y,nlevs),\n",
    "             batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(x3d_test,x2d_test,y_test,nlevs),\n",
    "             batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "     \n",
    "    # Sets the model into training mode\n",
    "    mlp.train()\n",
    "    train_loss = 0\n",
    "    # x=data in, y=data out, z = extra loss qnext\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = mlp(x)\n",
    "        phys_loss = loss_function(prediction, y)\n",
    "        # qnext_loss = q_loss_tensors_mm(prediction, z, x)\n",
    "        # qnext_loss = q_loss_tensors_std(prediction, z, x)\n",
    "        loss = phys_loss #+ 0.*qnext_loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "            batch_idx * len(x), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(x)))\n",
    "        \n",
    "        ##### Use this for finding learning rate \n",
    "        # scheduler.step()\n",
    "        # for param_group in optimizer.param_groups:\n",
    "        #     print(param_group['lr'], loss.item() / len(x))\n",
    "        ###### LR finder end\n",
    "    average_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.6f}'.format(epoch, average_loss))\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    mlp.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(validate_loader):\n",
    "            x = x.to(device)\n",
    "            prediction = mlp(x)\n",
    "            validation_loss += loss_function(prediction,y).item()\n",
    "            # if i == 0:\n",
    "            #     n = min(x.size(0), 8)\n",
    "            #     comparison = torch.cat([data[:n],\n",
    "            #                           recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "            #     save_image(comparison.cpu(),\n",
    "            #              'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    validation_loss /= len(validate_loader.dataset)\n",
    "    print('====> validation loss: {:.6f}'.format(validation_loss))\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_save(epoch: int, nn_model: model, nn_optimizer: torch.optim, training_loss: list, validation_loss: list, model_name: str):\n",
    "    \"\"\"\n",
    "    Save model checkpoints\n",
    "    \"\"\"\n",
    "    checkpoint_name = model_name.replace('.tar','_chkepo_{0}.tar'.format(str(epoch).zfill(3)))\n",
    "    torch.save({'epoch':epoch,\n",
    "                'model_state_dict':nn_model.state_dict(),\n",
    "                'optimizer_state_dict':nn_optimizer.state_dict(),\n",
    "                'training_loss':training_loss,\n",
    "                'validation_loss':validation_loss},\n",
    "                locations['model_loc']+'/'+checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main():\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(epoch)\n",
    "        validate_loss = validate(epoch)\n",
    "        scheduler.step()    \n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(validate_loss)\n",
    "        if epoch % 1 == 0:\n",
    "            checkpoint_save(epoch, mlp, optimizer, training_loss, validation_loss, model_name)\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save({'epoch':epoch,\n",
    "                'model_state_dict':mlp.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'training_loss':training_loss,\n",
    "                'validation_loss':validation_loss},\n",
    "                locations['model_loc']+'/'+model_name)"
   ]
  }
 ]
}